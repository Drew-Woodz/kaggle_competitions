{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18935d00",
   "metadata": {},
   "source": [
    "**1. Setup & Data Load**\n",
    "\n",
    "- Load train and test CSVs (with all engineered features)\n",
    "\n",
    "**2. Preprocessing for Modeling**\n",
    "\n",
    "- Encode categorical features (LabelEncoder or OneHotEncoder)\n",
    "\n",
    "- Separate X and y\n",
    "\n",
    "- Train/Val split (Stratified)\n",
    "\n",
    "**3. Baseline Modeling**\n",
    "\n",
    "- LightGBM (fast, accurate, and handles nulls + cats well)\n",
    "\n",
    "- Basic training with cross-validation\n",
    "\n",
    "- Output log loss and accuracy\n",
    "\n",
    "- Plot confusion matrix\n",
    "\n",
    "**4. Feature Importance**\n",
    "\n",
    "- LightGBM’s built-in importances\n",
    "\n",
    "- Optional: SHAP (if you want deeper analysis)\n",
    "\n",
    "**5. Submission**\n",
    "- Predict on test\n",
    "\n",
    "- Export predictions to CSV for Kaggle submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20089008",
   "metadata": {},
   "source": [
    "### **Load Data & Required Packages**\n",
    "\n",
    "Start by importing the core libraries needed for modeling:\n",
    "\n",
    "- `pandas` / `numpy` for data manipulation\n",
    "- `matplotlib` / `seaborn` for plotting\n",
    "- `sklearn` for cross-validation and metrics\n",
    "- `lightgbm` for efficient tree-based modeling\n",
    "\n",
    "I'll also load the cleaned and feature-engineered training and test sets from disk and confirm their shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b006c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (8693, 34)\n",
      "Test shape: (4277, 33)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, log_loss\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Load the feature-engineered datasets\n",
    "train = pd.read_csv('../data/train_clean.csv')\n",
    "test = pd.read_csv('../data/test_clean.csv')\n",
    "\n",
    "# Check initial shape\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a795346",
   "metadata": {},
   "source": [
    "**Separate Features and Target**\n",
    "\n",
    "We split the cleaned training dataset into:\n",
    "- `X`: features (everything except the target)\n",
    "- `y`: target label (`Transported`, converted to integer)\n",
    "\n",
    "The test dataset (`X_test`) is kept as-is (it has no label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3dbe84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = train.drop(columns=['Transported'])\n",
    "y = train['Transported'].astype(int)  # Convert boolean to int for modeling\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc2452a",
   "metadata": {},
   "source": [
    "**Set Up Cross-Validation**\n",
    "\n",
    "Using **StratifiedKFold** with 5 splits to:\n",
    "- Maintain class distribution in each fold\n",
    "- Get a reliable measure of model performance across different subsets\n",
    "- Shuffle the data before splitting (random_state ensures reproducibility)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb29157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up stratified k-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fc6be8",
   "metadata": {},
   "source": [
    "**Train Baseline LightGBM Model**\n",
    "\n",
    "Training a default **LightGBM classifier** using stratified 5-fold cross-validation.\n",
    "\n",
    "- Log loss is computed for each fold and averaged.\n",
    "- Out-of-fold (OOF) predictions help us evaluate performance without leaking validation data.\n",
    "- Test set predictions are averaged across all folds to prepare for submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9fa2931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000567 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1906\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n",
      "[LightGBM] [Info] Start training from score 0.014380\n",
      "Fold 1 Log Loss: 0.44683\n",
      "[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000442 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1906\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n",
      "[LightGBM] [Info] Start training from score 0.014380\n",
      "Fold 2 Log Loss: 0.44748\n",
      "[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1905\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n",
      "[LightGBM] [Info] Start training from score 0.014380\n",
      "Fold 3 Log Loss: 0.43954\n",
      "[LightGBM] [Info] Number of positive: 3503, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000653 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1907\n",
      "[LightGBM] [Info] Number of data points in the train set: 6955, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503666 -> initscore=0.014666\n",
      "[LightGBM] [Info] Start training from score 0.014666\n",
      "Fold 4 Log Loss: 0.43405\n",
      "[LightGBM] [Info] Number of positive: 3503, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1906\n",
      "[LightGBM] [Info] Number of data points in the train set: 6955, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503666 -> initscore=0.014666\n",
      "[LightGBM] [Info] Start training from score 0.014666\n",
      "Fold 5 Log Loss: 0.45521\n",
      "\n",
      "Overall CV Log Loss: 0.44462\n"
     ]
    }
   ],
   "source": [
    "# Drop non-numeric columns for baseline\n",
    "non_numeric_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "X = X.drop(columns=non_numeric_cols)\n",
    "test_X = test.drop(columns=non_numeric_cols)\n",
    "\n",
    "# Ensure alignment with test features\n",
    "test_X = test_X[X.columns]\n",
    "\n",
    "# Retrain model with clean features\n",
    "oof_preds = np.zeros(len(train))\n",
    "test_preds = np.zeros(len(test))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "    model = LGBMClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    oof_preds[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "    test_preds += model.predict_proba(test_X)[:, 1] / cv.n_splits\n",
    "\n",
    "    fold_loss = log_loss(y_val, oof_preds[val_idx])\n",
    "    print(f'Fold {fold+1} Log Loss: {fold_loss:.5f}')\n",
    "\n",
    "total_loss = log_loss(y, oof_preds)\n",
    "print(f'\\nOverall CV Log Loss: {total_loss:.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9148a",
   "metadata": {},
   "source": [
    "| Fold    | Log Loss    |\n",
    "| ------- | ----------- |\n",
    "| 1       | 0.44683     |\n",
    "| 2       | 0.44748     |\n",
    "| 3       | 0.43954     |\n",
    "| 4       | 0.43405     |\n",
    "| 5       | 0.45521     |\n",
    "| **Avg** | **0.44462** |\n",
    "\n",
    "**Interpretation**\n",
    "- Log loss of ~0.44 means the model is predicting well-calibrated probabilities, not just labels.\n",
    "\n",
    "- LightGBM is already separating the signal from noise with just the numeric features.\n",
    "\n",
    "- There's room for improvement—especially from:\n",
    "\n",
    "    - Categorical encoding\n",
    "\n",
    "    - Feature interactions (e.g., combining AgeGroup + VIP)\n",
    "\n",
    "    - Hyperparameter tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dba5490a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7952\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y, oof_preds > 0.5)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
