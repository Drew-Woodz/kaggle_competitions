{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be71e94d",
   "metadata": {},
   "source": [
    "Step-by-Step Plan for the Notebook\n",
    "\n",
    "Step 1: Load and Inspect Cleaned Data\n",
    "\n",
    "- Print shapes\n",
    "\n",
    "- Check Transported.value_counts(normalize=True) to see if the classes are balanced\n",
    "\n",
    "Step 2: Prep Features and Target\n",
    "\n",
    "- Drop PassengerId, Name, Surname, etc.\n",
    "\n",
    "- Split X and y\n",
    "\n",
    "- Encode categorical features (start with simple OneHotEncoder or pd.get_dummies)\n",
    "\n",
    "- Optional: Use a ColumnTransformer to prep the pipeline\n",
    "\n",
    "\n",
    "Step 3: Baseline Models (no tuning yet)\n",
    "\n",
    "Pick 3 diverse classifiers:\n",
    "\n",
    "1. RandomForestClassifier → Great benchmark, not too sensitive to scaling or encoding\n",
    "\n",
    "2. LightGBMClassifier → Fast, robust, leaderboard-class\n",
    "\n",
    "3. LogisticRegression (with regularization) → Simple linear baseline, tells you if nonlinear models are doing anything special\n",
    "\n",
    "Use StratifiedKFold or cross_val_score with scoring='accuracy' or 'roc_auc'.\n",
    "\n",
    "Step 4: Compare Results\n",
    "\n",
    "Print:\n",
    "\n",
    "- Mean + std dev of cross-val scores\n",
    "\n",
    "- Confusion matrix on train set for quick insight\n",
    "\n",
    "- Feature importances (where available)\n",
    "\n",
    "Step 5: Decide the Winner(s)\n",
    "Pick the one or two best models to:\n",
    "\n",
    "- Tune (grid/random search or Optuna)\n",
    "\n",
    "- Calibrate (optional)\n",
    "\n",
    "- Ensemble (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e4bec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (8693, 33)\n",
      "Test shape: (4277, 32)\n",
      "Categorical columns: ['PassengerId', 'HomePlanet', 'Cabin', 'Name', 'CabinDeck', 'CabinSide', 'Surname', 'VIP_confidence', 'Destination_confidence', 'Destination_cleaned', 'Age_confidence']\n",
      "Numeric columns: ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'GroupID', 'CabinNum', 'Cabin_missing', 'NoAmenities', 'CryoSleep_missing', 'RoomService_missing', 'FoodCourt_missing', 'ShoppingMall_missing', 'Spa_missing', 'VRDeck_missing', 'TotalSpend', 'GroupSize']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('../data/train_clean.csv')\n",
    "test = pd.read_csv('../data/test_clean.csv')\n",
    "\n",
    "\n",
    "# Check initial shape\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "\n",
    "# Separate features and labels\n",
    "X = train.drop(columns=['Transported'])\n",
    "y = train['Transported'].astype(int)  # Convert boolean to int for modeling\n",
    "X_test = test.copy()\n",
    "\n",
    "y.value_counts(normalize=True)\n",
    "X.dtypes.value_counts()\n",
    "\n",
    "cat_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "num_cols = X.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "print(\"Categorical columns:\", cat_cols)\n",
    "print(\"Numeric columns:\", num_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaf9059",
   "metadata": {},
   "source": [
    "Got 11 categorical columns:\n",
    "\n",
    "`PassengerId`, `Name`, `Surname`, and `Cabin` are identifiers or high-cardinality noise — drop these.\n",
    "\n",
    "`CabinDeck`, `CabinSide`, `HomePlanet`, `VIP_confidence`, `Destination_confidence`, `Destination_cleaned`, and `Age_confidence` are actual useful categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "479eed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop identifier columns that won’t help the model\n",
    "drop_cols = ['PassengerId', 'Name', 'Surname', 'Cabin']\n",
    "X = X.drop(columns=drop_cols)\n",
    "X_test = X_test.drop(columns=drop_cols)\n",
    "\n",
    "# Redefine categorical columns\n",
    "cat_cols = ['CabinDeck', 'CabinSide', 'HomePlanet', 'VIP_confidence',\n",
    "            'Destination_confidence', 'Destination_cleaned', 'Age_confidence']\n",
    "\n",
    "# Ensure all other numeric columns are included\n",
    "num_cols = [col for col in X.columns if col not in cat_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35be525e",
   "metadata": {},
   "source": [
    "Define the ColumnTransformer and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91223801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Preprocessing: encode categoricals and scale numerics\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n",
    "    ]\n",
    ")\n",
    "# Note: sparse_output=False gives us dense numpy arrays for compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd58b8",
   "metadata": {},
   "source": [
    "Define the Model Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc3754ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to test\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'LightGBM': LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Wrap each model with preprocessing pipeline\n",
    "pipelines = {\n",
    "    name: Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
    "    for name, model in models.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8854e08c",
   "metadata": {},
   "source": [
    "Cross-Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08236cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_models(X, y, pipelines, scoring='accuracy', cv=5):\n",
    "    results = {}\n",
    "    for name, pipe in pipelines.items():\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        scores = cross_val_score(pipe, X, y, cv=cv, scoring=scoring)\n",
    "        print(f\"  {scoring}: {np.mean(scores):.4f} ± {np.std(scores):.4f}\")\n",
    "        results[name] = scores\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d72eab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aac6372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RandomForest...\n",
      "  accuracy: 0.7728 ± 0.0368\n",
      "Evaluating LogisticRegression...\n",
      "  accuracy: 0.7832 ± 0.0100\n",
      "Evaluating LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000537 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2197\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n",
      "[LightGBM] [Info] Start training from score 0.014380\n",
      "[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2194\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n",
      "[LightGBM] [Info] Start training from score 0.014380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\source\\kaggle_competitions\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\andre\\source\\kaggle_competitions\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000428 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2198\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n",
      "[LightGBM] [Info] Start training from score 0.014380\n",
      "[LightGBM] [Info] Number of positive: 3503, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000380 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2197\n",
      "[LightGBM] [Info] Number of data points in the train set: 6955, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503666 -> initscore=0.014666\n",
      "[LightGBM] [Info] Start training from score 0.014666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\source\\kaggle_competitions\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\andre\\source\\kaggle_competitions\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3503, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000481 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2198\n",
      "[LightGBM] [Info] Number of data points in the train set: 6955, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503666 -> initscore=0.014666\n",
      "[LightGBM] [Info] Start training from score 0.014666\n",
      "  accuracy: 0.6804 ± 0.0761\n",
      "Evaluating RandomForest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\source\\kaggle_competitions\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  f1: 0.7402 ± 0.0726\n",
      "Evaluating LogisticRegression...\n",
      "  f1: 0.7921 ± 0.0095\n",
      "Evaluating LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2197\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n",
      "[LightGBM] [Info] Start training from score 0.014380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\source\\kaggle_competitions\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\andre\\source\\kaggle_competitions\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000380 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2194\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n",
      "[LightGBM] [Info] Start training from score 0.014380\n",
      "[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000367 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2198\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n",
      "[LightGBM] [Info] Start training from score 0.014380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\source\\kaggle_competitions\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\andre\\source\\kaggle_competitions\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3503, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000384 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2197\n",
      "[LightGBM] [Info] Number of data points in the train set: 6955, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503666 -> initscore=0.014666\n",
      "[LightGBM] [Info] Start training from score 0.014666\n",
      "[LightGBM] [Info] Number of positive: 3503, number of negative: 3452\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000850 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2198\n",
      "[LightGBM] [Info] Number of data points in the train set: 6955, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503666 -> initscore=0.014666\n",
      "[LightGBM] [Info] Start training from score 0.014666\n",
      "  f1: 0.5701 ± 0.2036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\source\\kaggle_competitions\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RandomForest': array([0.62295082, 0.72637431, 0.80607083, 0.82750846, 0.71813031]),\n",
       " 'LogisticRegression': array([0.77555556, 0.79847078, 0.79793341, 0.7875    , 0.80123584]),\n",
       " 'LightGBM': array([0.35580524, 0.64115523, 0.72669492, 0.82005013, 0.30694981])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_models(X, y, pipelines, scoring='accuracy')\n",
    "evaluate_models(X, y, pipelines, scoring='f1')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
