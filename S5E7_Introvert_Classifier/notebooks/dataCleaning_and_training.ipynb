{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b56d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "\n",
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0623af",
   "metadata": {},
   "source": [
    "**Data Cleaning & Modeling Pipeline Plan**\n",
    "\n",
    "1. Load and inspect data\n",
    "\n",
    "2. Handle missing values\n",
    "\n",
    "3. Feature engineering\n",
    "\n",
    "4. Encode target\n",
    "\n",
    "5. Train/test split\n",
    "\n",
    "6. Baseline model\n",
    "\n",
    "7. Evaluate and iterate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde67448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and inspect the data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "# Basic overview\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(\"\\nTrain info:\")\n",
    "train.info()\n",
    "\n",
    "# Missing values\n",
    "print(\"\\nMissing values in train:\")\n",
    "print(train.isna().sum()[train.isna().sum() > 0].sort_values(ascending=False))\n",
    "\n",
    "# Preview target\n",
    "print(\"\\nTarget value counts:\")\n",
    "print(train['Personality'].value_counts(dropna=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5c8636",
   "metadata": {},
   "source": [
    "**Dataset Summary**\n",
    "**Shapes:**\n",
    "\n",
    "    train: 18,524 rows, 9 columns\n",
    "\n",
    "    test: 6,175 rows, 8 columns (no target)\n",
    "\n",
    "|Column|Missing|Type|\n",
    "|--|--|--|\n",
    "|`Stage_fear`|1,893|object|\n",
    "|`Going_outside`|1,466|float|\n",
    "|`Post_frequency`|1,264|float|\n",
    "|`Time_spent_Alone`|1,190|float|\n",
    "|`Social_event_attendance`|1,180|float|\n",
    "|`Drained_after_socializing`|1,149|object|\n",
    "|`Friends_circle_size`|1,054|float|\n",
    "\n",
    "**Target distribution:**\n",
    "\n",
    " -   Extrovert: 13,699 (~74%)\n",
    "\n",
    " -   Introvert: 4,825 (~26%)\n",
    "\n",
    " -  **Imbalanced target**, something I'll need to handle during training\n",
    "\n",
    "**Next Steps** (**Step 2** Plan: Clean the Data)\n",
    "\n",
    "I'll handle missing values carefully based on our EDA findings:\n",
    "\n",
    "1. Numerical Columns (float):\n",
    "\n",
    "    - Impute using correlated features, KNN Imputer.\n",
    "\n",
    "    - Use linear correlation-based fill when there's a strong relationship\n",
    "\n",
    "    - These include: Time_spent_Alone, Social_event_attendance, Going_outside, Friends_circle_size, Post_frequency\n",
    "\n",
    "2. Categorical Columns (object):\n",
    "\n",
    "    - For Stage_fear and Drained_after_socializing, I observed they correlate with missingness in numeric fields\n",
    "\n",
    "    - So I can fill them using related categorical/numeric values (like Going_outside, Post_frequency) grouped mode\n",
    "\n",
    "3. Outlier handling (optional but worth flagging for later â€” I might revisit this during model tuning).\n",
    "\n",
    "4. Encode categorical variables:\n",
    "\n",
    "    - Stage_fear, Drained_after_socializing, and Personality (target)\n",
    "\n",
    "5. Create a was_missing_* binary flag for imputed values\n",
    "\n",
    "    - Always a good idea as it gives models a shot at capturing patterns related to why data was missing.\n",
    "\n",
    "6. Save cleaned dataset for training reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988639cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add binary flags for each column with missing data\n",
    "missing_cols = [\n",
    "    'Time_spent_Alone',\n",
    "    'Stage_fear',\n",
    "    'Social_event_attendance',\n",
    "    'Going_outside',\n",
    "    'Drained_after_socializing',\n",
    "    'Friends_circle_size',\n",
    "    'Post_frequency'\n",
    "]\n",
    "\n",
    "for col in missing_cols:\n",
    "    train[f'{col}_missing'] = train[col].isna().astype(int)\n",
    "    test[f'{col}_missing'] = test[col].isna().astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39727f47",
   "metadata": {},
   "source": [
    "**Step 3: Fill Missing Values**\n",
    "\n",
    "- Numeric Columns -> KNN Imputer\n",
    "\n",
    "- Categorical Columns ->   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c091713",
   "metadata": {},
   "source": [
    "**Define Groups of Correlated Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d684bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Logical groupings based on correlation structure\n",
    "group_social_behavior = ['Time_spent_Alone', 'Going_outside', 'Drained_after_socializing', 'Stage_fear']\n",
    "group_social_networking = ['Friends_circle_size', 'Post_frequency']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f9dbd9",
   "metadata": {},
   "source": [
    "**KNN Imputer for Numerical Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9d3d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Combine train and test for joint imputation\n",
    "combined = pd.concat([train, test], keys=['train', 'test'])\n",
    "\n",
    "# Numeric columns to impute and scale\n",
    "knn_impute_cols = ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside',\n",
    "                   'Friends_circle_size', 'Post_frequency']\n",
    "\n",
    "# Impute missing values\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "combined[knn_impute_cols] = knn_imputer.fit_transform(combined[knn_impute_cols])\n",
    "\n",
    "# Split combined back into train and test\n",
    "train = combined.xs('train').copy()\n",
    "test = combined.xs('test').copy()\n",
    "\n",
    "# Scale numeric columns\n",
    "scaler = StandardScaler()\n",
    "train.loc[:, knn_impute_cols] = scaler.fit_transform(train[knn_impute_cols])\n",
    "test.loc[:, knn_impute_cols] = scaler.transform(test[knn_impute_cols])\n",
    "\n",
    "train[knn_impute_cols].describe().T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eaea8e",
   "metadata": {},
   "source": [
    "**Predict Categorical Columns (Binary Classification)**\n",
    "\n",
    "But first, before we forget again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c98d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean string-based binary columns while keeping NaNs intact\n",
    "binary_map = {'Yes': 1, 'No': 0}\n",
    "for col in ['Stage_fear', 'Drained_after_socializing']:\n",
    "    train.loc[:, col] = train[col].apply(lambda x: binary_map[x] if x in binary_map else np.nan)\n",
    "    test.loc[:, col] = test[col].apply(lambda x: binary_map[x] if x in binary_map else np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06299a87",
   "metadata": {},
   "source": [
    "I'll predict missing values in `Stage_fear` and `Drained_after_socializing` by using logistic regression trained only on the rows with complete values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def impute_categorical(train_df, test_df, column, predictors, verbose=False):\n",
    "    # Only use rows with known target\n",
    "    known = train_df[train_df[column].notnull()]\n",
    "    unknown = train_df[train_df[column].isnull()]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n--- Imputing {column} ---\")\n",
    "        print(\"Label distribution:\\n\", known[column].value_counts())\n",
    "        print(\"Predictors nulls:\\n\", known[predictors].isnull().sum())\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(known[predictors], known[column].astype(int))\n",
    "\n",
    "    # Predict and fill missing values in train\n",
    "    if not unknown.empty:\n",
    "        train_df.loc[train_df[column].isnull(), column] = model.predict(unknown[predictors])\n",
    "\n",
    "    # Predict and fill missing values in test\n",
    "    test_unknown = test_df[test_df[column].isnull()]\n",
    "    if not test_unknown.empty:\n",
    "        test_df.loc[test_df[column].isnull(), column] = model.predict(test_unknown[predictors])\n",
    "\n",
    "\n",
    "# Define predictor sets based on correlation analysis\n",
    "predictors_stage_fear = ['Time_spent_Alone', 'Going_outside', 'Social_event_attendance']\n",
    "predictors_drained = ['Time_spent_Alone', 'Stage_fear', 'Going_outside']\n",
    "\n",
    "impute_categorical(train, test, 'Stage_fear', predictors_stage_fear, verbose=True)\n",
    "impute_categorical(train, test, 'Drained_after_socializing', predictors_drained, verbose=True)\n",
    "\n",
    "# Ensure binary categorical columns are cast to int after model prediction (they may be float)\n",
    "train.loc[:, 'Stage_fear'] = train['Stage_fear'].astype(int)\n",
    "test.loc[:, 'Stage_fear'] = test['Stage_fear'].astype(int)\n",
    "\n",
    "train.loc[:, 'Drained_after_socializing'] = train['Drained_after_socializing'].astype(int)\n",
    "test.loc[:, 'Drained_after_socializing'] = test['Drained_after_socializing'].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d557fb3c",
   "metadata": {},
   "source": [
    "**Encoding Categorical Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1c643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe encoding of target and binary columns with .loc to avoid SettingWithCopyWarning\n",
    "train.loc[:, 'Personality'] = train['Personality'].map({'Introvert': 0, 'Extrovert': 1})\n",
    "test.loc[:, 'Personality'] = test['Personality'].map({'Introvert': 0, 'Extrovert': 1})  # Only if used in eval\n",
    "\n",
    "# Drop rows with missing target in training set\n",
    "train = train[train['Personality'].notnull()].copy()\n",
    "train['Personality'] = train['Personality'].astype(int)\n",
    "\n",
    "binary_cols = ['Stage_fear', 'Drained_after_socializing']\n",
    "for col in binary_cols:\n",
    "    train.loc[:, col] = train[col].astype(int)\n",
    "    test.loc[:, col] = test[col].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb916a5",
   "metadata": {},
   "source": [
    "Let's take a moment to validate our janitorial efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908afd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Confirm no missing values\n",
    "print(\"Missing values in train:\\n\", train.isna().sum())\n",
    "print(\"\\nMissing values in test:\\n\", test.isna().sum())\n",
    "\n",
    "# 2. Check dtypes and unique values of categorical (now numeric) columns\n",
    "print(\"\\nStage_fear unique:\", train['Stage_fear'].unique())\n",
    "print(\"Drained_after_socializing unique:\", train['Drained_after_socializing'].unique())\n",
    "print(\"Personality unique:\", train['Personality'].unique())\n",
    "\n",
    "# 3. Check target distribution\n",
    "print(\"\\nTarget value counts:\")\n",
    "print(train['Personality'].value_counts())\n",
    "\n",
    "# 4. Sample preview\n",
    "print(\"\\nTrain sample:\")\n",
    "print(train.head())\n",
    "\n",
    "# Show columns and data types\n",
    "train.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f8366",
   "metadata": {},
   "source": [
    "- spotless\n",
    "\n",
    "---\n",
    "\n",
    "Fix label formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ae89bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.copy()\n",
    "test = test.copy()\n",
    "\n",
    "# Convert to int\n",
    "train['Drained_after_socializing'] = train['Drained_after_socializing'].astype(int)\n",
    "test['Drained_after_socializing'] = test['Drained_after_socializing'].astype(int)\n",
    "\n",
    "train['Stage_fear'] = train['Stage_fear'].astype(int)\n",
    "test['Stage_fear'] = test['Stage_fear'].astype(int)\n",
    "\n",
    "# Encode labels\n",
    "label_map = {'Introvert': 0, 'Extrovert': 1}\n",
    "train['Personality'] = train['Personality'].map(label_map)\n",
    "test['Personality'] = test['Personality'].map(label_map)\n",
    "\n",
    "# Show columns and data types\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e4c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned datasets\n",
    "train.to_csv(\"../data/cleaned_train.csv\", index=False)\n",
    "test.to_csv(\"../data/cleaned_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a44bc6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Split the Training Set\n",
    "\n",
    "- `X_train`, `X_valid`\n",
    "\n",
    "- `y_train`, `y_valid`\n",
    "\n",
    "For model evaluation before final test predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65b7218",
   "metadata": {},
   "source": [
    "Prepare training features and target by separating predictors from the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fe19ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['Personality'].notnull()].copy()\n",
    "train['Personality'] = train['Personality'].astype(int)\n",
    "\n",
    "\n",
    "X = train.drop(columns=['Personality', 'id'])  # keep ID only for post-pred join\n",
    "y = train['Personality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0125a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\n",
    "    'Time_spent_Alone', 'Social_event_attendance', 'Going_outside',\n",
    "    'Friends_circle_size', 'Post_frequency'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a81386",
   "metadata": {},
   "source": [
    "Standardize numeric feature columns to have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a0db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[num_cols] = scaler.fit_transform(X[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c116e2",
   "metadata": {},
   "source": [
    "Create interaction features to capture relationships between social behavior and binary traits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f2eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled['Alone_x_Fear'] = X_scaled['Time_spent_Alone'] * X_scaled['Stage_fear']\n",
    "X_scaled['Social_x_Drained'] = X_scaled['Social_event_attendance'] * X_scaled['Drained_after_socializing']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21db1a4a",
   "metadata": {},
   "source": [
    "Split the data into training and validation sets using stratified sampling to preserve class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebee665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b4f2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "y_proba = clf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"AUC:\", roc_auc_score(y_val, y_proba))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4f5c52",
   "metadata": {},
   "source": [
    "Holy Crap! 95%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
