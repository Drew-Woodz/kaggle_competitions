{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b56d36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "Device: cuda\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "\n",
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0623af",
   "metadata": {},
   "source": [
    "**Data Cleaning & Modeling Pipeline Plan**\n",
    "\n",
    "1. Load and inspect data\n",
    "\n",
    "2. Handle missing values\n",
    "\n",
    "3. Feature engineering\n",
    "\n",
    "4. Encode target\n",
    "\n",
    "5. Train/test split\n",
    "\n",
    "6. Baseline model\n",
    "\n",
    "7. Evaluate and iterate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fde67448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (18524, 9)\n",
      "Test shape: (6175, 8)\n",
      "\n",
      "Train info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18524 entries, 0 to 18523\n",
      "Data columns (total 9 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   id                         18524 non-null  int64  \n",
      " 1   Time_spent_Alone           17334 non-null  float64\n",
      " 2   Stage_fear                 16631 non-null  object \n",
      " 3   Social_event_attendance    17344 non-null  float64\n",
      " 4   Going_outside              17058 non-null  float64\n",
      " 5   Drained_after_socializing  17375 non-null  object \n",
      " 6   Friends_circle_size        17470 non-null  float64\n",
      " 7   Post_frequency             17260 non-null  float64\n",
      " 8   Personality                18524 non-null  object \n",
      "dtypes: float64(5), int64(1), object(3)\n",
      "memory usage: 1.3+ MB\n",
      "\n",
      "Missing values in train:\n",
      "Stage_fear                   1893\n",
      "Going_outside                1466\n",
      "Post_frequency               1264\n",
      "Time_spent_Alone             1190\n",
      "Social_event_attendance      1180\n",
      "Drained_after_socializing    1149\n",
      "Friends_circle_size          1054\n",
      "dtype: int64\n",
      "\n",
      "Target value counts:\n",
      "Personality\n",
      "Extrovert    13699\n",
      "Introvert     4825\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and inspect the data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "if 'Personality' in test.columns:\n",
    "    test = test.drop(columns=['Personality'])\n",
    "\n",
    "# Basic overview\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(\"\\nTrain info:\")\n",
    "train.info()\n",
    "\n",
    "# Missing values\n",
    "print(\"\\nMissing values in train:\")\n",
    "print(train.isna().sum()[train.isna().sum() > 0].sort_values(ascending=False))\n",
    "\n",
    "# Preview target\n",
    "print(\"\\nTarget value counts:\")\n",
    "print(train['Personality'].value_counts(dropna=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5c8636",
   "metadata": {},
   "source": [
    "**Dataset Summary**\n",
    "**Shapes:**\n",
    "\n",
    "    train: 18,524 rows, 9 columns\n",
    "\n",
    "    test: 6,175 rows, 8 columns (no target)\n",
    "\n",
    "|Column|Missing|Type|\n",
    "|--|--|--|\n",
    "|`Stage_fear`|1,893|object|\n",
    "|`Going_outside`|1,466|float|\n",
    "|`Post_frequency`|1,264|float|\n",
    "|`Time_spent_Alone`|1,190|float|\n",
    "|`Social_event_attendance`|1,180|float|\n",
    "|`Drained_after_socializing`|1,149|object|\n",
    "|`Friends_circle_size`|1,054|float|\n",
    "\n",
    "**Target distribution:**\n",
    "\n",
    " -   Extrovert: 13,699 (~74%)\n",
    "\n",
    " -   Introvert: 4,825 (~26%)\n",
    "\n",
    " -  **Imbalanced target**, something I'll need to handle during training\n",
    "\n",
    "**Next Steps** (**Step 2** Plan: Clean the Data)\n",
    "\n",
    "I'll handle missing values carefully based on our EDA findings:\n",
    "\n",
    "1. Numerical Columns (float):\n",
    "\n",
    "    - Impute using correlated features, KNN Imputer.\n",
    "\n",
    "    - Use linear correlation-based fill when there's a strong relationship\n",
    "\n",
    "    - These include: Time_spent_Alone, Social_event_attendance, Going_outside, Friends_circle_size, Post_frequency\n",
    "\n",
    "2. Categorical Columns (object):\n",
    "\n",
    "    - For Stage_fear and Drained_after_socializing, I observed they correlate with missingness in numeric fields\n",
    "\n",
    "    - So I can fill them using related categorical/numeric values (like Going_outside, Post_frequency) grouped mode\n",
    "\n",
    "3. Outlier handling (optional but worth flagging for later — I might revisit this during model tuning).\n",
    "\n",
    "4. Encode categorical variables:\n",
    "\n",
    "    - Stage_fear, Drained_after_socializing, and Personality (target)\n",
    "\n",
    "5. Create a was_missing_* binary flag for imputed values\n",
    "\n",
    "    - Always a good idea as it gives models a shot at capturing patterns related to why data was missing.\n",
    "\n",
    "6. Save cleaned dataset for training reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "988639cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add binary flags for each column with missing data\n",
    "missing_cols = [\n",
    "    'Time_spent_Alone',\n",
    "    'Stage_fear',\n",
    "    'Social_event_attendance',\n",
    "    'Going_outside',\n",
    "    'Drained_after_socializing',\n",
    "    'Friends_circle_size',\n",
    "    'Post_frequency'\n",
    "]\n",
    "\n",
    "for col in missing_cols:\n",
    "    train[f'{col}_missing'] = train[col].isna().astype(int)\n",
    "    test[f'{col}_missing'] = test[col].isna().astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39727f47",
   "metadata": {},
   "source": [
    "**Step 3: Fill Missing Values**\n",
    "\n",
    "- Numeric Columns -> KNN Imputer\n",
    "\n",
    "- Categorical Columns ->   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c091713",
   "metadata": {},
   "source": [
    "**Define Groups of Correlated Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58d684bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Logical groupings based on correlation structure\n",
    "group_social_behavior = ['Time_spent_Alone', 'Going_outside', 'Drained_after_socializing', 'Stage_fear']\n",
    "group_social_networking = ['Friends_circle_size', 'Post_frequency']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f9dbd9",
   "metadata": {},
   "source": [
    "**KNN Imputer for Numerical Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb9d3d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Time_spent_Alone</th>\n",
       "      <td>18524.0</td>\n",
       "      <td>7.211295e-17</td>\n",
       "      <td>1.000027</td>\n",
       "      <td>-1.050602</td>\n",
       "      <td>-0.714208</td>\n",
       "      <td>-0.377813</td>\n",
       "      <td>0.294975</td>\n",
       "      <td>2.649735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Social_event_attendance</th>\n",
       "      <td>18524.0</td>\n",
       "      <td>3.028360e-16</td>\n",
       "      <td>1.000027</td>\n",
       "      <td>-1.905702</td>\n",
       "      <td>-0.810873</td>\n",
       "      <td>-0.080987</td>\n",
       "      <td>0.648899</td>\n",
       "      <td>1.743728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Going_outside</th>\n",
       "      <td>18524.0</td>\n",
       "      <td>4.345956e-16</td>\n",
       "      <td>1.000027</td>\n",
       "      <td>-1.956687</td>\n",
       "      <td>-0.492413</td>\n",
       "      <td>-0.004321</td>\n",
       "      <td>0.971861</td>\n",
       "      <td>1.459953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Friends_circle_size</th>\n",
       "      <td>18524.0</td>\n",
       "      <td>1.077859e-16</td>\n",
       "      <td>1.000027</td>\n",
       "      <td>-1.900660</td>\n",
       "      <td>-0.707331</td>\n",
       "      <td>0.008666</td>\n",
       "      <td>0.724663</td>\n",
       "      <td>1.679326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Post_frequency</th>\n",
       "      <td>18524.0</td>\n",
       "      <td>4.660491e-17</td>\n",
       "      <td>1.000027</td>\n",
       "      <td>-1.720321</td>\n",
       "      <td>-0.672331</td>\n",
       "      <td>0.026330</td>\n",
       "      <td>0.724990</td>\n",
       "      <td>1.772981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           count          mean       std       min       25%  \\\n",
       "Time_spent_Alone         18524.0  7.211295e-17  1.000027 -1.050602 -0.714208   \n",
       "Social_event_attendance  18524.0  3.028360e-16  1.000027 -1.905702 -0.810873   \n",
       "Going_outside            18524.0  4.345956e-16  1.000027 -1.956687 -0.492413   \n",
       "Friends_circle_size      18524.0  1.077859e-16  1.000027 -1.900660 -0.707331   \n",
       "Post_frequency           18524.0  4.660491e-17  1.000027 -1.720321 -0.672331   \n",
       "\n",
       "                              50%       75%       max  \n",
       "Time_spent_Alone        -0.377813  0.294975  2.649735  \n",
       "Social_event_attendance -0.080987  0.648899  1.743728  \n",
       "Going_outside           -0.004321  0.971861  1.459953  \n",
       "Friends_circle_size      0.008666  0.724663  1.679326  \n",
       "Post_frequency           0.026330  0.724990  1.772981  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Combine train and test for joint imputation\n",
    "combined = pd.concat([train, test], keys=['train', 'test'])\n",
    "\n",
    "# Numeric columns to impute and scale\n",
    "knn_impute_cols = ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside',\n",
    "                   'Friends_circle_size', 'Post_frequency']\n",
    "\n",
    "# Impute missing values\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "combined[knn_impute_cols] = knn_imputer.fit_transform(combined[knn_impute_cols])\n",
    "\n",
    "# Split combined back into train and test\n",
    "train = combined.xs('train').copy()\n",
    "test = combined.xs('test').copy()\n",
    "\n",
    "# Scale numeric columns\n",
    "scaler = StandardScaler()\n",
    "train.loc[:, knn_impute_cols] = scaler.fit_transform(train[knn_impute_cols])\n",
    "test.loc[:, knn_impute_cols] = scaler.transform(test[knn_impute_cols])\n",
    "\n",
    "train[knn_impute_cols].describe().T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eaea8e",
   "metadata": {},
   "source": [
    "**Predict Categorical Columns (Binary Classification)**\n",
    "\n",
    "But first, before we forget again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c98d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean string-based binary columns while keeping NaNs intact\n",
    "binary_map = {'Yes': 1, 'No': 0}\n",
    "for col in ['Stage_fear', 'Drained_after_socializing']:\n",
    "    train.loc[:, col] = train[col].apply(lambda x: binary_map[x] if x in binary_map else np.nan)\n",
    "    test.loc[:, col] = test[col].apply(lambda x: binary_map[x] if x in binary_map else np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06299a87",
   "metadata": {},
   "source": [
    "I'll predict missing values in `Stage_fear` and `Drained_after_socializing` by using logistic regression trained only on the rows with complete values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2e1bc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Imputing Stage_fear ---\n",
      "Label distribution:\n",
      " Stage_fear\n",
      "0.0    12609\n",
      "1.0     4022\n",
      "Name: count, dtype: int64\n",
      "Predictors nulls:\n",
      " Time_spent_Alone           0\n",
      "Going_outside              0\n",
      "Social_event_attendance    0\n",
      "dtype: int64\n",
      "\n",
      "--- Imputing Drained_after_socializing ---\n",
      "Label distribution:\n",
      " Drained_after_socializing\n",
      "0.0    13313\n",
      "1.0     4062\n",
      "Name: count, dtype: int64\n",
      "Predictors nulls:\n",
      " Time_spent_Alone    0\n",
      "Stage_fear          0\n",
      "Going_outside       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def impute_categorical(train_df, test_df, column, predictors, verbose=False):\n",
    "    # Only use rows with known target\n",
    "    known = train_df[train_df[column].notnull()]\n",
    "    unknown = train_df[train_df[column].isnull()]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n--- Imputing {column} ---\")\n",
    "        print(\"Label distribution:\\n\", known[column].value_counts())\n",
    "        print(\"Predictors nulls:\\n\", known[predictors].isnull().sum())\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(known[predictors], known[column].astype(int))\n",
    "\n",
    "    # Predict and fill missing values in train\n",
    "    if not unknown.empty:\n",
    "        train_df.loc[train_df[column].isnull(), column] = model.predict(unknown[predictors])\n",
    "\n",
    "    # Predict and fill missing values in test\n",
    "    test_unknown = test_df[test_df[column].isnull()]\n",
    "    if not test_unknown.empty:\n",
    "        test_df.loc[test_df[column].isnull(), column] = model.predict(test_unknown[predictors])\n",
    "\n",
    "\n",
    "# Define predictor sets based on correlation analysis\n",
    "predictors_stage_fear = ['Time_spent_Alone', 'Going_outside', 'Social_event_attendance']\n",
    "predictors_drained = ['Time_spent_Alone', 'Stage_fear', 'Going_outside']\n",
    "\n",
    "impute_categorical(train, test, 'Stage_fear', predictors_stage_fear, verbose=True)\n",
    "impute_categorical(train, test, 'Drained_after_socializing', predictors_drained, verbose=True)\n",
    "\n",
    "# Ensure binary categorical columns are cast to int after model prediction (they may be float)\n",
    "train.loc[:, 'Stage_fear'] = train['Stage_fear'].astype(int)\n",
    "test.loc[:, 'Stage_fear'] = test['Stage_fear'].astype(int)\n",
    "\n",
    "train.loc[:, 'Drained_after_socializing'] = train['Drained_after_socializing'].astype(int)\n",
    "test.loc[:, 'Drained_after_socializing'] = test['Drained_after_socializing'].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d557fb3c",
   "metadata": {},
   "source": [
    "**Encoding Categorical Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a1c643d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 18524 entries, 0 to 18523\n",
      "Data columns (total 16 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   id                                 18524 non-null  int64  \n",
      " 1   Time_spent_Alone                   18524 non-null  float64\n",
      " 2   Stage_fear                         18524 non-null  object \n",
      " 3   Social_event_attendance            18524 non-null  float64\n",
      " 4   Going_outside                      18524 non-null  float64\n",
      " 5   Drained_after_socializing          18524 non-null  object \n",
      " 6   Friends_circle_size                18524 non-null  float64\n",
      " 7   Post_frequency                     18524 non-null  float64\n",
      " 8   Personality                        18524 non-null  int64  \n",
      " 9   Time_spent_Alone_missing           18524 non-null  int64  \n",
      " 10  Stage_fear_missing                 18524 non-null  int64  \n",
      " 11  Social_event_attendance_missing    18524 non-null  int64  \n",
      " 12  Going_outside_missing              18524 non-null  int64  \n",
      " 13  Drained_after_socializing_missing  18524 non-null  int64  \n",
      " 14  Friends_circle_size_missing        18524 non-null  int64  \n",
      " 15  Post_frequency_missing             18524 non-null  int64  \n",
      "dtypes: float64(5), int64(9), object(2)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Safe encoding of target and binary columns with .loc to avoid SettingWithCopyWarning\n",
    "train.loc[:, 'Personality'] = train['Personality'].map({'Introvert': 0, 'Extrovert': 1})\n",
    "test.loc[:, 'Personality'] = test['Personality'].map({'Introvert': 0, 'Extrovert': 1})  # Only if used in eval\n",
    "\n",
    "# Drop rows with missing target in training set\n",
    "train = train[train['Personality'].notnull()].copy()\n",
    "train['Personality'] = train['Personality'].astype(int)\n",
    "\n",
    "binary_cols = ['Stage_fear', 'Drained_after_socializing']\n",
    "for col in binary_cols:\n",
    "    train.loc[:, col] = train[col].astype(int)\n",
    "    test.loc[:, col] = test[col].astype(int)\n",
    "\n",
    "# Show columns and data types\n",
    "train.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb916a5",
   "metadata": {},
   "source": [
    "Let's take a moment to validate our janitorial efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "908afd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in train:\n",
      " id                                   0\n",
      "Time_spent_Alone                     0\n",
      "Stage_fear                           0\n",
      "Social_event_attendance              0\n",
      "Going_outside                        0\n",
      "Drained_after_socializing            0\n",
      "Friends_circle_size                  0\n",
      "Post_frequency                       0\n",
      "Personality                          0\n",
      "Time_spent_Alone_missing             0\n",
      "Stage_fear_missing                   0\n",
      "Social_event_attendance_missing      0\n",
      "Going_outside_missing                0\n",
      "Drained_after_socializing_missing    0\n",
      "Friends_circle_size_missing          0\n",
      "Post_frequency_missing               0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in test:\n",
      " id                                      0\n",
      "Time_spent_Alone                        0\n",
      "Stage_fear                              0\n",
      "Social_event_attendance                 0\n",
      "Going_outside                           0\n",
      "Drained_after_socializing               0\n",
      "Friends_circle_size                     0\n",
      "Post_frequency                          0\n",
      "Personality                          6175\n",
      "Time_spent_Alone_missing                0\n",
      "Stage_fear_missing                      0\n",
      "Social_event_attendance_missing         0\n",
      "Going_outside_missing                   0\n",
      "Drained_after_socializing_missing       0\n",
      "Friends_circle_size_missing             0\n",
      "Post_frequency_missing                  0\n",
      "dtype: int64\n",
      "\n",
      "Stage_fear unique: [0 1]\n",
      "Drained_after_socializing unique: [0 1]\n",
      "Personality unique: [1 0]\n",
      "\n",
      "Target value counts:\n",
      "Personality\n",
      "1    13699\n",
      "0     4825\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train sample:\n",
      "   id  Time_spent_Alone Stage_fear  Social_event_attendance  Going_outside  \\\n",
      "0   0         -1.050602          0                 0.283956      -0.004321   \n",
      "1   1         -0.714208          0                 0.648899      -0.492413   \n",
      "2   2          0.967764          1                -1.540759      -1.956687   \n",
      "3   3         -0.041419          0                 0.648899      -0.492413   \n",
      "4   4         -0.714208          0                -0.445930      -0.004321   \n",
      "\n",
      "  Drained_after_socializing  Friends_circle_size  Post_frequency  Personality  \\\n",
      "0                         0             1.679326        0.026330            1   \n",
      "1                         0             0.485997        1.074321            1   \n",
      "2                         1            -1.184663       -1.720321            0   \n",
      "3                         0             0.724663        0.026330            1   \n",
      "4                         0             1.201994        0.375660            1   \n",
      "\n",
      "   Time_spent_Alone_missing  Stage_fear_missing  \\\n",
      "0                         0                   0   \n",
      "1                         0                   0   \n",
      "2                         0                   0   \n",
      "3                         0                   0   \n",
      "4                         0                   0   \n",
      "\n",
      "   Social_event_attendance_missing  Going_outside_missing  \\\n",
      "0                                0                      0   \n",
      "1                                0                      0   \n",
      "2                                0                      0   \n",
      "3                                0                      0   \n",
      "4                                0                      0   \n",
      "\n",
      "   Drained_after_socializing_missing  Friends_circle_size_missing  \\\n",
      "0                                  0                            0   \n",
      "1                                  0                            0   \n",
      "2                                  1                            0   \n",
      "3                                  0                            0   \n",
      "4                                  0                            0   \n",
      "\n",
      "   Post_frequency_missing  \n",
      "0                       0  \n",
      "1                       0  \n",
      "2                       0  \n",
      "3                       0  \n",
      "4                       1  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 18524 entries, 0 to 18523\n",
      "Data columns (total 16 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   id                                 18524 non-null  int64  \n",
      " 1   Time_spent_Alone                   18524 non-null  float64\n",
      " 2   Stage_fear                         18524 non-null  object \n",
      " 3   Social_event_attendance            18524 non-null  float64\n",
      " 4   Going_outside                      18524 non-null  float64\n",
      " 5   Drained_after_socializing          18524 non-null  object \n",
      " 6   Friends_circle_size                18524 non-null  float64\n",
      " 7   Post_frequency                     18524 non-null  float64\n",
      " 8   Personality                        18524 non-null  int64  \n",
      " 9   Time_spent_Alone_missing           18524 non-null  int64  \n",
      " 10  Stage_fear_missing                 18524 non-null  int64  \n",
      " 11  Social_event_attendance_missing    18524 non-null  int64  \n",
      " 12  Going_outside_missing              18524 non-null  int64  \n",
      " 13  Drained_after_socializing_missing  18524 non-null  int64  \n",
      " 14  Friends_circle_size_missing        18524 non-null  int64  \n",
      " 15  Post_frequency_missing             18524 non-null  int64  \n",
      "dtypes: float64(5), int64(9), object(2)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# 1. Confirm no missing values\n",
    "print(\"Missing values in train:\\n\", train.isna().sum())\n",
    "print(\"\\nMissing values in test:\\n\", test.isna().sum())\n",
    "\n",
    "# 2. Check dtypes and unique values of categorical (now numeric) columns\n",
    "print(\"\\nStage_fear unique:\", train['Stage_fear'].unique())\n",
    "print(\"Drained_after_socializing unique:\", train['Drained_after_socializing'].unique())\n",
    "print(\"Personality unique:\", train['Personality'].unique())\n",
    "\n",
    "# 3. Check target distribution\n",
    "print(\"\\nTarget value counts:\")\n",
    "print(train['Personality'].value_counts())\n",
    "\n",
    "# 4. Sample preview\n",
    "print(\"\\nTrain sample:\")\n",
    "print(train.head())\n",
    "\n",
    "# Show columns and data types\n",
    "train.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f8366",
   "metadata": {},
   "source": [
    "- spotless\n",
    "\n",
    "---\n",
    "\n",
    "Fix label formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9ae89bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 18524 entries, 0 to 18523\n",
      "Data columns (total 16 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   id                                 18524 non-null  int64  \n",
      " 1   Time_spent_Alone                   18524 non-null  float64\n",
      " 2   Stage_fear                         18524 non-null  int64  \n",
      " 3   Social_event_attendance            18524 non-null  float64\n",
      " 4   Going_outside                      18524 non-null  float64\n",
      " 5   Drained_after_socializing          18524 non-null  int64  \n",
      " 6   Friends_circle_size                18524 non-null  float64\n",
      " 7   Post_frequency                     18524 non-null  float64\n",
      " 8   Personality                        18524 non-null  int64  \n",
      " 9   Time_spent_Alone_missing           18524 non-null  int64  \n",
      " 10  Stage_fear_missing                 18524 non-null  int64  \n",
      " 11  Social_event_attendance_missing    18524 non-null  int64  \n",
      " 12  Going_outside_missing              18524 non-null  int64  \n",
      " 13  Drained_after_socializing_missing  18524 non-null  int64  \n",
      " 14  Friends_circle_size_missing        18524 non-null  int64  \n",
      " 15  Post_frequency_missing             18524 non-null  int64  \n",
      "dtypes: float64(5), int64(11)\n",
      "memory usage: 2.4 MB\n"
     ]
    }
   ],
   "source": [
    "train = train.copy()\n",
    "test = test.copy()\n",
    "\n",
    "# Convert to int\n",
    "train['Drained_after_socializing'] = train['Drained_after_socializing'].astype(int)\n",
    "test['Drained_after_socializing'] = test['Drained_after_socializing'].astype(int)\n",
    "\n",
    "train['Stage_fear'] = train['Stage_fear'].astype(int)\n",
    "test['Stage_fear'] = test['Stage_fear'].astype(int)\n",
    "\n",
    "# Show columns and data types\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1e4c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned datasets\n",
    "train.to_csv(\"../data/cleaned_train.csv\", index=False)\n",
    "test.to_csv(\"../data/cleaned_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a44bc6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Split the Training Set\n",
    "\n",
    "- `X_train`, `X_valid`\n",
    "\n",
    "- `y_train`, `y_valid`\n",
    "\n",
    "For model evaluation before final test predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65b7218",
   "metadata": {},
   "source": [
    "Prepare training features and target by separating predictors from the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31fe19ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['Personality'].notnull()].copy()\n",
    "train['Personality'] = train['Personality'].astype(int)\n",
    "\n",
    "\n",
    "X = train.drop(columns=['Personality', 'id'])  # keep ID only for post-pred join\n",
    "y = train['Personality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0125a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\n",
    "    'Time_spent_Alone', 'Social_event_attendance', 'Going_outside',\n",
    "    'Friends_circle_size', 'Post_frequency'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a81386",
   "metadata": {},
   "source": [
    "Standardize numeric feature columns to have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09a0db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[num_cols] = scaler.fit_transform(X[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c116e2",
   "metadata": {},
   "source": [
    "Create interaction features to capture relationships between social behavior and binary traits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3f2eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled['Alone_x_Fear'] = X_scaled['Time_spent_Alone'] * X_scaled['Stage_fear']\n",
    "X_scaled['Social_x_Drained'] = X_scaled['Social_event_attendance'] * X_scaled['Drained_after_socializing']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21db1a4a",
   "metadata": {},
   "source": [
    "Split the data into training and validation sets using stratified sampling to preserve class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebee665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1 classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94       965\n",
      "           1       0.98      0.98      0.98      2740\n",
      "\n",
      "    accuracy                           0.97      3705\n",
      "   macro avg       0.96      0.96      0.96      3705\n",
      "weighted avg       0.97      0.97      0.97      3705\n",
      "\n",
      "AUC: 0.9619\n",
      "\n",
      "Fold 2 classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       965\n",
      "           1       0.97      0.98      0.98      2740\n",
      "\n",
      "    accuracy                           0.97      3705\n",
      "   macro avg       0.96      0.95      0.96      3705\n",
      "weighted avg       0.97      0.97      0.97      3705\n",
      "\n",
      "AUC: 0.9655\n",
      "\n",
      "Fold 3 classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93       965\n",
      "           1       0.97      0.98      0.98      2740\n",
      "\n",
      "    accuracy                           0.97      3705\n",
      "   macro avg       0.96      0.95      0.96      3705\n",
      "weighted avg       0.97      0.97      0.97      3705\n",
      "\n",
      "AUC: 0.9578\n",
      "\n",
      "Fold 4 classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94       965\n",
      "           1       0.98      0.98      0.98      2740\n",
      "\n",
      "    accuracy                           0.97      3705\n",
      "   macro avg       0.97      0.96      0.96      3705\n",
      "weighted avg       0.97      0.97      0.97      3705\n",
      "\n",
      "AUC: 0.9656\n",
      "\n",
      "Fold 5 classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94       965\n",
      "           1       0.98      0.98      0.98      2739\n",
      "\n",
      "    accuracy                           0.97      3704\n",
      "   macro avg       0.96      0.96      0.96      3704\n",
      "weighted avg       0.97      0.97      0.97      3704\n",
      "\n",
      "AUC: 0.9687\n",
      "\n",
      "Mean AUC over folds: 0.9639\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "auc_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_scaled, y)):\n",
    "    X_train, X_val = X_scaled.iloc[train_idx], X_scaled.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_val)\n",
    "    y_proba = clf.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    auc = roc_auc_score(y_val, y_proba)\n",
    "    auc_scores.append(auc)\n",
    "    \n",
    "    print(f\"\\nFold {fold+1} classification report:\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "print(f\"\\nMean AUC over folds: {np.mean(auc_scores):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4f5c52",
   "metadata": {},
   "source": [
    "Holy Crap! 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e059a654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Submission file saved as 'submission_rf_baseline.csv'\n"
     ]
    }
   ],
   "source": [
    "# 1. Capture feature names from training (pre-interactions)\n",
    "features = X.columns.tolist()  # Still 14 columns\n",
    "\n",
    "# 2. Prepare test features (copy to avoid modifying original)\n",
    "test_scaled = test[features].copy()\n",
    "\n",
    "# 3. Scale ONLY the numeric columns using the fitted scaler\n",
    "test_scaled[num_cols] = scaler.transform(test_scaled[num_cols])\n",
    "\n",
    "# 4. Add the same interaction features as in training\n",
    "test_scaled['Alone_x_Fear'] = test_scaled['Time_spent_Alone'] * test_scaled['Stage_fear']\n",
    "test_scaled['Social_x_Drained'] = test_scaled['Social_event_attendance'] * test_scaled['Drained_after_socializing']\n",
    "\n",
    "# Now test_scaled has 16 columns, matching X_scaled\n",
    "\n",
    "# Train on full data\n",
    "final_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "final_model.fit(X_scaled, y)\n",
    "\n",
    "# 5. Predict\n",
    "test_preds = final_model.predict(test_scaled)\n",
    "\n",
    "# 6. Map predictions back to labels if Kaggle expects strings (check competition page)\n",
    "# Assuming it wants 0/1, but if strings: test_preds = np.where(test_preds == 0, 'Introvert', 'Extrovert')\n",
    "\n",
    "# 7. Save submission file\n",
    "submission = pd.DataFrame({'id': test['id'], 'Personality': test_preds})\n",
    "# Map back from 0/1 to string labels\n",
    "submission['Personality'] = submission['Personality'].map({0: 'Introvert', 1: 'Extrovert'})\n",
    "submission.to_csv('../submissions/submission_rf_baseline.csv', index=False)\n",
    "\n",
    "print(\"✅ Submission file saved as 'submission_rf_baseline.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfdfd34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6175, 2)\n",
      "['id', 'Personality']\n",
      "object\n",
      "['Extrovert' 'Introvert']\n"
     ]
    }
   ],
   "source": [
    "print(submission.shape)                       # (6175, 2)\n",
    "print(submission.columns.tolist())            # ['id', 'Personality']\n",
    "print(submission['Personality'].dtype)        # int64\n",
    "print(submission['Personality'].unique())     # array([0, 1])\n",
    "assert not submission['Personality'].isna().any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff677f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✅ Shape:\", submission.shape)\n",
    "print(\"✅ Columns:\", submission.columns.tolist())\n",
    "print(\"✅ Dtypes:\\n\", submission.dtypes)\n",
    "print(\"✅ Unique values:\", submission['Personality'].unique())\n",
    "print(\"✅ Any NaNs?\", submission.isna().any().any())\n",
    "print(\"✅ Any duplicates?\", submission.duplicated(subset='id').any())\n",
    "print(\"✅ Any missing IDs?\", submission['id'].isna().any())\n",
    "print(\"✅ ID range preview:\", submission['id'].min(), \"to\", submission['id'].max())\n",
    "print(\"✅ Sorted by ID?\", submission['id'].is_monotonic_increasing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c3895b",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
